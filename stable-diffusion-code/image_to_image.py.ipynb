{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import PIL\n",
    "from diffusers import AutoencoderKL, DDIMScheduler, DiffusionPipeline, PNDMScheduler, UNet2DConditionModel\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "\n",
    "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModel,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        unet: UNet2DConditionModel,\n",
    "        scheduler: Union[DDIMScheduler, PNDMScheduler],\n",
    "        safety_checker: StableDiffusionSafetyChecker,\n",
    "        feature_extractor: CLIPFeatureExtractor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        scheduler = scheduler.set_format(\"pt\")\n",
    "        self.register_modules(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=safety_checker,\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        init_image: torch.FloatTensor,\n",
    "        strength: float = 0.8,\n",
    "        num_inference_steps: Optional[int] = 50,\n",
    "        guidance_scale: Optional[float] = 7.5,\n",
    "        eta: Optional[float] = 0.0,\n",
    "        generator: Optional[torch.Generator] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "    ):\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        # set timesteps\n",
    "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
    "        extra_set_kwargs = {}\n",
    "        offset = 0\n",
    "        if accepts_offset:\n",
    "            offset = 1\n",
    "            extra_set_kwargs[\"offset\"] = 1\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
    "\n",
    "        # encode the init image into latents and scale the latents\n",
    "        init_latents = self.vae.encode(init_image.to(self.device)).sample()\n",
    "        init_latents = 0.18215 * init_latents\n",
    "\n",
    "        # prepare init_latents noise to latents\n",
    "        init_latents = torch.cat([init_latents] * batch_size)\n",
    "\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = int(num_inference_steps * strength) + offset\n",
    "        init_timestep = min(init_timestep, num_inference_steps)\n",
    "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
    "        timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # add noise to latents using the timesteps\n",
    "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device)\n",
    "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
    "\n",
    "        # get prompt text embeddings\n",
    "        text_input = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            max_length = text_input.input_ids.shape[-1]\n",
    "            uncond_input = self.tokenizer(\n",
    "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "            )\n",
    "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
    "        # and should be between [0, 1]\n",
    "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
    "        extra_step_kwargs = {}\n",
    "        if accepts_eta:\n",
    "            extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "        latents = init_latents\n",
    "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
    "        for i, t in tqdm(enumerate(self.scheduler.timesteps[t_start:])):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
    "\n",
    "        # scale and decode the image latents with vae\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        image = self.vae.decode(latents)\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "        # run safety checker\n",
    "        safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
    "        image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
    "\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
